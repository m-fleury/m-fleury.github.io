<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width">
		

		<title>Blog</title>

		
		<link rel="stylesheet" href="https://m-fleury.github.io/css/colors-dark.min.91821db55d8acc5b9f936944224ab03a094cea2c707bacc60689050c84476d08.css">

		
	</head>
	<body>
		<header id="header">
			<h1><a href="https://m-fleury.github.io/">Webpage of Mathias Fleury</a></h1>
			<p></p>
		</header>

		<div id="page">
			<div id="sidebar">
				<nav>
	
		<ul class="nav">
			
				<li><a href="/duties/"><span>About me</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/blog/"><span>Blog</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/isasat/"><span>IsaSATs</span></a></li>
			
				<li><a href="/pasteque/"><span>Pasteques</span></a></li>
			
				<li><a href="/sources/"><span>Sources</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/sources/sources/"><span>Programs</span></a></li>
			
		</ul>
	
		<ul class="nav">
			
				<li><a href="/research/"><span>My Research</span></a></li>
			
		</ul>
	
</nav>

			</div>

			<div id="content">
				
	<article class="post">
		<h1><a href="https://m-fleury.github.io/blog/">Blog</a> </h1>

		<div class="post-content"><h2 id="artifacts-reviewing">Artifacts Reviewing</h2>
<p>I have consistently felt unease about our approach to examining
artifacts. Although I have personally reviewed numerous artifacts, I
believe that, as a community, we are mistaken in our methods.</p>
<h3 id="weird-errors-and-artifact-problems">Weird Errors and Artifact Problems</h3>
<p>The objective of artifact creation is straightforward: to enable the
reproducibility of results by allowing the same code to be run again
in the future. This can be achieved by packaging all necessary
components in a repository, such as Zenodo, ensuring that everything
is self-contained and does not rely on external internet
connections. Indeed, this approach seems quite appealing.</p>
<p>Yeah but not so much.</p>
<ol>
<li>Zenodo can delete artifacts without notifying users, like reported
by <a href="https://x.com/pruvisto/status/1580822617824845830?s=20">Manuel Eberl</a>.</li>
<li>I encountered a behaviour difference whether the directory was put
into the VM or shared. Another reviewer and I encountered an
installation issue, but the artifact&rsquo;s authors successfully
identified the error source (impressive!). Although this was
undoubtedly frustrating for them, I am concerned that similar
elusive issues may arise again or go unnoticed in the opposite
scenario (i.e., if the folder only functions properly when shared).</li>
<li>I encountered another issue: a VM created using a VirtualBox
version that is too recent (specifically, the latest release), and
it is incompatible with the version included in the Ubuntu LTS
distribution due to settings file differences. The two versions in
question are likely separated by approximately two years. It does
not appear that VirtualBox prioritizes compatibility with older
versions, as this is not a stated goal of the project. Actually
VirtualBox was also not running on my home computer, because the
CPU was <strong>too new</strong> (thanks to the Archlinux wiki, I found the fix)</li>
<li>It is unclear whether any artifact is useful or not. The idea is
that, in the future, we will be able to use binaries. However, this
is not obvious. For instance, if you want to run code on a cluster,
such as the entire SAT Competition or even the entire SMT-Lib, it
is not feasible to use VirtualBox on any cluster. Therefore, you
would need to export the binary, which assumes compatibility with
newer processors. Do you have an Apple M1 or a newer model? If so,
you might be lucky with Rosetta, but this may not be the case. To
make matters worse, running VirtualBox is currently broken on Apple
computers. So if in 10 years we all run RISC-V CPU&hellip;</li>
<li>I currently do not know anyone who used an artifact. Good
maintained software does not need it. Bad maintained
software&hellip; probably should not be used in the future anyway.</li>
</ol>
<h3 id="reviewing-is-broken">Reviewing is Broken</h3>
<p>We publish artifacts with nice badges like from the <a href="https://www.acm.org/publications/policies/artifact-review-badging">ACM</a>. Sounds great
right?</p>
<p>Yeah but not so much.</p>
<ol>
<li>It&rsquo;s not truly possible to reject an artifact. The worst badge one
can receive indicates that the artifact was uploaded, which says
nothing about its content. I tried once to reject an artifact
because it was really bad, but failed (as no other reviewer had
any comment to do after I clearly said I wanted to reject it).</li>
</ol>
<!--listend-->
<ol>
<li>Reviewers pre-review the artifact, informing authors of what they
need to fix during this process. Afterward, authors address the
issues and reviewers conduct a second pass. I had to review an
artifact where the authors did not even test it. However, I did during
the pre-review, basically doing it for them. Then, the authors
fixed the problems. Surprisingly, I had no problems with the
artifact anymore (and the artifact was accepted).</li>
<li>Reviews are not what one might expect: in most cases, the reviewer
has little knowledge about the artifact&rsquo;s topic, unlike the paper
review. At best, as a reviewer, I can comment on the scripts, but I
cannot comment on the actual code since it is not my area of
expertise. Moreover, as a reviewer, I must read the paper to
understand what is claimed often without really understanding the
topic.</li>
<li>Many conferences review the artifact simultaneously with the paper
using different reviewers who do not communicate with each
other. Therefore, even if a bug is found in the artifact, it is
unlikely to affect the paper&rsquo;s acceptance.</li>
<li>Zenodo is not ideal for rejected papers. As a reviewer, I can see
the various versions, enabling me to guess where it was previously
submitted. A few years ago, only the final version of the artifact
had to be on Zenodo or another platform. Now, even the reviewing
version must be included.</li>
<li>Few people seem willing to review artifacts. In fact, I believe
that at least 30% of all previews are missing, and many artifacts
are accepted with only two reviewers, as the third never reviewed
the artifact. To address this issue, artifact evaluation committees
are considering younger and younger PhD students (and yes, I am
aware that for machine learning conferences, it is normal for
first-year PhD students to review, but not in our community). My
impression is that the review quality is not increasing and the
number of missing reviews is increasing over time. And more and
more late PhD students do not want to review anymore.</li>
</ol>
<h3 id="what-should-we-do">What Should we Do?</h3>
<p>I really do not know, but here is what I think is important:</p>
<ol>
<li>drop the available badge.</li>
<li>past reviews of artifact must be included in the submission. Great
artifacts remain great.</li>
<li>pre-reviewing should be able to lead to a rejection.</li>
<li>reviewers of the paper should also review the artifact (but only if
it has a chance to be accepted).</li>
<li>tool papers should only be accepted with artifact if both the paper
and the artifact are good enough.</li>
</ol>
<p>I am not convinced that these points are enough, but it would be a start.</p>
<p>On my side, I will continue to review artifacts, but there is one
artifact committe I will never be part again, after failing to reject
that one artifact.</p>
</div>

		<p class="meta">Posted on <span class="postdate">01. January 0001</span></p>
	</article>

			</div>

			<footer id="footer">
				<p class="copyright">
					
						Powered by <a href="https://gohugo.io/">Hugo</a> and the
						<a href="https://github.com/bake/solar-theme-hugo">Solar</a>-theme.
					
				</p>
			</footer>
		</div>

		
	</body>
</html>
